{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRLND - P1 - Navigation : Report\n",
    "\n",
    "---\n",
    "\n",
    "In this report, I am going to present about the environment and the algorithms that I have used to solve the navigation problem where the agent is to collect bananas.\n",
    "\n",
    "### Environment\n",
    "\n",
    "The project uses an environment built in Unity. The nvironment contain **_brains_** which are responsible for deciding the actions of their associated agents. Below are the characteristics of the environment.\n",
    "\n",
    "* Unity brain name: BananaBrain\n",
    "* Vector Observation space type: continuous\n",
    "* Vector Observation space size (per agent): 37\n",
    "* Number of stacked Vector Observation: 1\n",
    "* Vector Action space type: discrete\n",
    "* Vector Action space size (per agent): 4\n",
    "\n",
    "## Learning Algorithm\n",
    "\n",
    "### DQN\n",
    "\n",
    "To solve this reinforment learning problem, I am using a Deep Q Network (DQN) as has been taught in the course. The agent uses replay buffer of length 10000 and a simple neural network. The agent uses **discount factor 0.99** and **learning rate 0.0005**\n",
    "\n",
    "#### Neural Network\n",
    "To approximate the value function a simple neural network  with three fully connected layers with ReLU activation function, is used.\n",
    "\n",
    "###### Network Architecture\n",
    "\n",
    "`` State --> 128 --> ReLU --> 128 --> ReLU --> action``\n",
    "\n",
    "###### Pytorch Implementation \n",
    "\n",
    "```python\n",
    "self.fc1 = nn.Linear(state_size, 128)\n",
    "self.fc2 = nn.Linear(128,128)\n",
    "self.fc3 = nn.Linear(128, action_size)\n",
    "\n",
    "...\n",
    "\n",
    "x = F.relu(self.fc1(state))\n",
    "x = F.relu(self.fc2(x))\n",
    "x = self.fc3(x)\n",
    "```\n",
    "\n",
    "#### Experience Replay\n",
    "\n",
    "We store the last **10,000** experience tuples (S, A, R, Sâ€²) into a data container called **replay buffer** from which we sample **a mini-batch of 64** experiences. This batch ensures that the experiences are not highly correlated/independet and stable enough to train the network. \n",
    "\n",
    "```python\n",
    "BUFFER_SIZE = int(1e4)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "```\n",
    "\n",
    "#### Epsilon Greedy\n",
    "\n",
    "In order to select the next action, the agent uses epsilon-greedy policy. The agent selects an action from action space randomly with a probability of **__epsilon__**, and is reduced gradually with decay rate **0.995** with the number of iterations till it reaches **0.01**\n",
    "\n",
    "```python\n",
    "eps_start=1.0, eps_end=0.01, eps_decay=0.995  \n",
    "```\n",
    "  \n",
    "  <br>\n",
    "  \n",
    "> **Parameters**\n",
    "```python\n",
    "BUFFER_SIZE = int(1e4)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate\n",
    "UPDATE_EVERY = 4        # how often to update the network```\n",
    ">\n",
    "\n",
    "\n",
    "## Plot of Rewards\n",
    "\n",
    "If the agent achieves the score over 13, the problem is deemed to be solved. After tuning of the parameters, I could solve the problem in **791  episodes**. The plot below shows the rewards per episode and moving average over last 100 episodes. \n",
    "\n",
    "![plot of rewards](./data/plot.jpg)\n",
    "\n",
    "Trained model can be found [here](./data/model_weights.pth).\n",
    "\n",
    "## Ideas for Future Work\n",
    "\n",
    "- To improve the performance, I am planning to implement [Double DQN](https://arxiv.org/abs/1509.06461), [Dueling DQN](https://arxiv.org/abs/1511.06581), [Prioritized Experienced Replay](https://arxiv.org/abs/1511.05952).\n",
    "- Train network from image data directly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

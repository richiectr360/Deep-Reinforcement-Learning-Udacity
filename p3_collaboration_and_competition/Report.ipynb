{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRLND - P3 - Collaboration and Competition : Report\n",
    "________________________________________________________________________\n",
    "\n",
    "In this report, I am going to present about the environment and the algorithms that I have used to solve collaboration and competition problem where the agents must bounce ball back and forth while not dropping or sending ball out of bounds.\n",
    "\n",
    "## Environment\n",
    "\n",
    "We work with Unity ML Environment, [Tennis](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#tennis), in this project.\n",
    "\n",
    "In this environment, there are two agents which control rackets to bounce a ball over a net. The agent receives +0.1 reward if it manages to hit ball over the net without dropping or hitting out of bounds. For dropping or hitting out of bounds, it receives -0.01 . \n",
    "\n",
    "The observation space consists of 24 variables representing position and velocity of the ball and racket. Each action is a vector with two numbers, corresponding to movement towards or away from the net, and jumping. The action vector should be a number between 1 and -1.\n",
    "\n",
    "The environment is deemed solved if the agents get an average score of +0.5 over 100 consecutive episodes. \n",
    "\n",
    "Given below are the characteristics of the agents and the environment.\n",
    "\n",
    "* Unity brain name: TennisBrain\n",
    "* Number of Visual Observations (per agent): 0\n",
    "* Vector Observation space type: continuous\n",
    "* Vector Observation space size (per agent): 8\n",
    "* Number of stacked Vector Observation: 3\n",
    "* Vector Action space type: continuous\n",
    "* Vector Action space size (per agent): 2\n",
    "\n",
    "\n",
    "\n",
    "## Learning Algorithm\n",
    "\n",
    "To solve this reinforment learning problem, I am using a Deep Deterministic Policy Gradients (DDPG) with modification to make it suitable for multiagent environment.\n",
    "\n",
    "### Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "**DDPG** is an actor-critic algorithm that extends **DQN** to work in continuous spaces. Here, we use two deep neural networks, one as actor and the other as critic. Similar network architectures are used for both actor and critic. **ADAM** optimizer is used with **learning rates 0.0001** and **0.001** for actor and critic, respectively. And the **discount factor** used is **0.99**.\n",
    "\n",
    "```python\n",
    "GAMMA = 0.99            # discount factor\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "```\n",
    "##### Neural Network Architecture\n",
    "\n",
    "###### Actor\n",
    "\n",
    "State --> BatchNorm --> 128 --> ReLU --> 64 --> ReLU --> BatchNorm --> action --> tanh\n",
    "\n",
    "###### Critic\n",
    "\n",
    "State --> BatchNorm --> 128 --> Relu --> 64 --> Relu --> action\n",
    "\n",
    "##### Pytorch Implementation\n",
    "\n",
    "###### Actor\n",
    "\n",
    "```python\n",
    "    self.fc1 = nn.Linear(state_size, 128)\n",
    "    self.bn1 = nn.BatchNorm1d(128)\n",
    "    self.fc2 = nn.Linear(128, 64)\n",
    "    self.bn2 = nn.BatchNorm1d(64)\n",
    "    self.fc3 = nn.Linear(64, 2)\n",
    "    self.bn3 = nn.BatchNorm1d(2)\n",
    "\n",
    "    ...\n",
    "\n",
    "    x = self.fc1(state)\n",
    "    x = F.relu(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.fc2(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.bn2(x)\n",
    "    x = self.fc3(x)\n",
    "    x = self.bn3(x) \n",
    "    x = F.tanh(x)\n",
    "```\n",
    "\n",
    "###### Critic\n",
    "\n",
    "```python\n",
    "self.bn0 = nn.BatchNorm1d(state_size)\n",
    "self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "self.bn1 = nn.BatchNorm1d(fcs1_units)\n",
    "self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "self.bn2 = nn.BatchNorm1d(fc2_units)\n",
    "self.fc3 = nn.Linear(fc2_units, 1)\n",
    "\n",
    "...\n",
    "\n",
    "x = self.bn0(state)\n",
    "x = self.fcs1(x)\n",
    "x = F.relu(x)\n",
    "x = torch.cat((x, action), dim=1)\n",
    "x = self.fc2(x)\n",
    "x = F.relu(x)\n",
    "x = self.fc3(x)\n",
    "```\n",
    "#### Experience Replay\n",
    "\n",
    "We store the last one million experience tuples (S,A,R,S') into a data container called **Replay Buffer** from which we sample **a mini batch of 1024** experiences. This batch ensures that the experiences are independent and stable enough to train the network.\n",
    "\n",
    "```python\n",
    "    BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "    BATCH_SIZE = 1024       # minibatch size\n",
    "```\n",
    "\n",
    "#### Soft Target Updates\n",
    "\n",
    "In order to calculate the target values for both actor and critic networks, we use **Soft Target Update** strategy. \n",
    "\n",
    "```python\n",
    "    TAU = 1e-3              # for soft update of target parameters\n",
    "```\n",
    "\n",
    "> In order for the agents to explore the entire environment, replay memory buffer is shared between the agents. Yet, | | > each agent has its own actor and critic networks to train.\n",
    "\n",
    "\n",
    "\n",
    "## Plot of Rewards\n",
    "\n",
    "After tuning the parameters and tweaking the network by changing the hidden layers size, I could solve the problem in **904 episodes**. The plot below shows the rewards per episode and the target.\n",
    "\n",
    "\n",
    "![Plot of Rewards](data/plot.jpg)\n",
    "\n",
    "## Result\n",
    "\n",
    "![Result](tennis_result.gif)\n",
    "\n",
    "Trained models can be found [here](weights/). And, final weights that solved the environment are stored in current folder(_*_solved.pth_)\n",
    "\n",
    "## Ideas for Future Work\n",
    "\n",
    "* After training, when I checked the performance, it seems that the agents are imitating each other. Reason could be the symmetry of the environment. Multi-Agent DDPG would be apt for environment like [Soccer](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#soccer-twos). So, I am planning to implement and see the performance of MADDPG in Soccer environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
